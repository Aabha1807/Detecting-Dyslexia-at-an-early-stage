{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e53a6f-7fea-4d73-bab1-b45858a19a6a",
   "metadata": {},
   "source": [
    "# Pattern Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49004c1-5352-44b0-9a51-1536de1ed311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb2520-5eed-4a77-bb68-b7142e1707b8",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c71601-e59f-47a6-af5f-4ad0c5804ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzklEQVR4nGNgGHCgcP21NoLHhCqZpCHigEuSLxGPqTP+//+/CIdc9P/////vwy6n//b/lT3Iksh2Ogkx9J7BYaj26//zGDr+Z2PV6SDCsBZVOSOc5bxMjIHhKrPG+TsMT4rQTZ3zHwHQdWpcZ2D40iuj4Myw8eiR42ganf//72NgYDDF5k+Wp//fi6BLwlwbKsVw8A3ERzwYOh///+nOwMDAwPDwvxCGzrUPOncyMDAwMJxkCMa0FAb6/u/ALWn9/z/cUiZM6Zk/cGulPQAAx4xLDpAJOxoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7FDD46381B20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = 'CN_dataset/Train'\n",
    "validation_dir = 'CN_dataset/Test'\n",
    "\n",
    "try_img = PIL.Image.open(train_dir+'/Normal/A-0.png')\n",
    "try_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ddb34d-15b1-47e4-93aa-f2a59cdafdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode of the data:  L\n",
      "Size of Images:  (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Mode of the data: \", try_img.mode)\n",
    "print(\"Size of Images: \", try_img._size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff57c83-308f-4871-a055-9ddebefaea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal:  39334\n",
      "Corrected:  65534\n",
      "Corrected:  2\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "print(\"Normal: \", len(glob.glob(train_dir+'/Normal/*')))\n",
    "print(\"Corrected: \", len(glob.glob(train_dir+'/Corrected/*')))\n",
    "print(\"Corrected: \", len(glob.glob(train_dir+'/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "533b56a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Aabha/Desktop/Detecting-Dyslexia-at-an-early-stage\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ddb81f-ff52-49c3-b4cd-0c3092151a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 104868 images belonging to 2 classes.\n",
      "Found 38842 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = try_img._size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                    rotation_range=10,\n",
    "                                    # zoom_range = 0.1,\n",
    "                                    # width_shift_range=0.1,\n",
    "                                    # height_shift_range=0.1\n",
    "                                  )\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,   #path to the training dataset\n",
    "    target_size = IMAGE_SIZE,\n",
    "    batch_size = BATCH_SIZE, \n",
    "    class_mode ='binary')  #binary.. since we have two classes only\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size = IMAGE_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17671b63-192e-4207-8d6c-060e34175545",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231a513b-52aa-42ee-b7a4-e3ddb29c762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 1)         28        \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 6280      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,317\n",
      "Trainable params: 6,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = IMAGE_SIZE + (3, )\n",
    "print(INPUT_SHAPE)\n",
    "\n",
    "model = tf.keras.models.Sequential([                               \n",
    "    tf.keras.layers.Conv2D(1, (3, 3), activation='relu', input_shape=INPUT_SHAPE, padding=\"same\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378bb4d5-ca75-4e45-b3c5-a23230877dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e5b42c9-c120-4583-923d-1774b5ef09a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = 86115//BATCH_SIZE\n",
    "VALIDATION_STEPS = 37439//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d1a7632-e1de-4fd0-a1a2-13289c770541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aabha/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2691/2691 [==============================] - 125s 46ms/step - loss: 0.3920 - accuracy: 0.8255 - val_loss: 0.3766 - val_accuracy: 0.8377\n",
      "Epoch 2/5\n",
      "2691/2691 [==============================] - 125s 46ms/step - loss: 0.3327 - accuracy: 0.8564 - val_loss: 0.4265 - val_accuracy: 0.8209\n",
      "Epoch 3/5\n",
      "2691/2691 [==============================] - 172s 64ms/step - loss: 0.3142 - accuracy: 0.8638 - val_loss: 0.4202 - val_accuracy: 0.8204\n",
      "Epoch 4/5\n",
      "2691/2691 [==============================] - 173s 64ms/step - loss: 0.3086 - accuracy: 0.8654 - val_loss: 0.3267 - val_accuracy: 0.8585\n",
      "Epoch 5/5\n",
      "2691/2691 [==============================] - 127s 47ms/step - loss: 0.3050 - accuracy: 0.8675 - val_loss: 0.3622 - val_accuracy: 0.8451\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, \n",
    "            epochs=5,\n",
    "            steps_per_epoch = STEPS_PER_EPOCH,\n",
    "            validation_data = validation_generator,\n",
    "            validation_steps = VALIDATION_STEPS, \n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97494d8f-6e34-4d9c-ac9a-6209ea198808",
   "metadata": {},
   "source": [
    "# Testing with custom input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fa0c512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdd339468e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "try_img = PIL.Image.open(validation_dir+'/Normal/A-102.png')\n",
    "try_img\n",
    "\n",
    "import cv2\n",
    "img = cv2.imread(validation_dir+'/Corrected/4_1.png')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "555122b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.resize(img,(28,28))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "890e426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img = np.reshape(img,[1,28,28,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "689b5c46-bfda-42fe-ba47-01bd0f524864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.539459]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fcdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
